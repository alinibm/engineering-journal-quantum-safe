---
title: EKS Cluster build
description: Running Guardium Insights with QSPM on EKS
sidebar:
  label: Cluster Build
  order: 2
---

:::note[Useful Links]

Some of these steps come from this documentation for [Guardium Insights on EKS](https://www.ibm.com/docs/en/guardium-insights/3.x?topic=scenarios-installation-amazon-elastic-kubernetes-service-eks)

:::


:::note
This assumes you have already configured `eksctl` and your aws configuration with your account info.

This also assumes you will be creating the cluster in the us-east region. This can be set to whatever region you want.
:::

Run the `eksctl` command below to create your first cluster and perform the following:

-   Create a 4-node Kubernetes cluster named `gi-east` with node type as `m6i.4xlarge` and region as `us-east-1`.
-   Control plane is managed by AWS EKS, so sizing for control plane nodes doesn't apply.
-   Define a minimum of one node (`--nodes-min 4`) and a maximum of three-node (`--nodes-max 6`) for this node group managed by EKS.
-   Create a node group with the name `guardium-workers` and select a machine type for the `guardium-workers` node group.

:::caution[Multiple AZ support]
As of this writing, GI v3.4.0 does not support multiple availability zones. 
:::

## Creating the cluster

Let's set some env vars to make our lives easier. 
:::note[On cluster names]

We're deploying in `us-east-1` and we are naming our cluster `gi-east`. This name will be exclusive to this cluster, so `gi-east` is an example here. You aren't bound to using `gi-east` as your name.

:::

```bash
export clustername=gi-east
export region=us-east-1
```

```tsx
eksctl create cluster \
--name ${clustername} \
--version 1.31 \
--region ${region} \
--zones ${region}a,${region}b \
--nodegroup-name guardium-workers \
--node-type m6i.4xlarge \
--with-oidc \
--nodes 4 \
--nodes-min 4 \
--nodes-max 6 \
--node-zones ${region}a \
--tags "Product=Guardium" \
--managed
```

Associate an IAM oidc provider with the cluster if you didn't include `--with-oidc` above.
```tsx
eksctl utils associate-iam-oidc-provider --region=${region} --cluster=${clustername} --approve
```
## Configure `kubectl`

Once the cluster is up, add it to your kube config. `eksctl` will probably do this for you.

```tsx
aws eks update-kubeconfig --name ${clustername} --region ${region}
```

{/*
### Create the EKS Pod Identity Agent Addon

We're going to use the EKS pod identity management addon to as IRSA (IAM Roles for Service Accounts) is deprecated and Pod Identity Management requires less permissions for a user who may not have IAM privileges.

```tsx
eksctl create addon \
--name eks-pod-identity-agent \
--cluster ${clustername} \
--region ${region} \
--version latest
```

Let's verify the addon was created

```tsx
eksctl get addon --cluster ${clustername} --region ${region} --name eks-pod-identity-agent -o json 
```

Should return something like this
```tsx
2024-08-23 12:09:50 [ℹ]  Kubernetes version "1.30" in use by cluster "gi-east"
2024-08-23 12:09:51 [ℹ]  to see issues for an addon run `eksctl get addon --name <addon-name> --cluster <cluster-name>`
[
    {
        "Name": "eks-pod-identity-agent",
        "Version": "v1.3.0-eksbuild.1",
        "NewerVersion": "",
        "IAMRole": "",
        "Status": "ACTIVE",
        "ConfigurationValues": "",
        "Issues": null,
        "PodIdentityAssociations": null
    }
]
```
*/}

## Configure Security Constraints (Optional)

For some environments there might be security constraints applied to the EKS cluster. Follow
these instructions to apply security constraints using OPA gatekeeper.

If the operating environment is a production or pre-production enterprise environment, then
you can skip this section as it's meant for development environments.

### Install OPA Gatekeeper

Deploy OPA Gatekeeper using prebuilt docker images

```tsx
kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper/release-3.17/deploy/gatekeeper.yaml
```

Check the pods in gatekeeper-system namespace

```tsx
kubectl get pods -n gatekeeper-system
```

The output will be similar to:

```tsx
NAME                                             READY   STATUS    RESTARTS   AGE
gatekeeper-audit-5bc9b59c57-9d9hc                1/1     Running   0          25s
gatekeeper-controller-manager-744cdc8556-hxf2n   1/1     Running   0          25s
gatekeeper-controller-manager-744cdc8556-jn42m   1/1     Running   0          25s
gatekeeper-controller-manager-744cdc8556-wwrb6   1/1     Running   0          25s
```

### Install kustomize
To ease in the deployment of the templates and constraints, install [kustomize](https://kubectl.docs.kubernetes.io/installation/kustomize/).

The following script detects your OS and downloads the appropriate kustomize binary to your current working directory.

```tsx
curl -s "https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh"  | bash
```

### Apply Templates

There are 2 parts to OPA gatekeeper policy enforcment, the first part is the `ConstraintTemplate`.
This is where the logic of the policy exists. The templates must be deployed prior to deploying the 
constraints.

If you haven't yet, clone this github repository.

```
git clone https://github.com/ibm-client-engineering/engineering-journal-quantum-safe.git
```

Go to the opa library templates directory.

```
cd engineering-journal-quantum-safe/opa/library/templates
```

Apply the templates using `kustomize`

```
kustomize build . | kubectl apply -f -
```

Sample output

```
constrainttemplate.templates.gatekeeper.sh/k8sallowedrepos created
constrainttemplate.templates.gatekeeper.sh/k8sblockendpointeditdefaultrole created
constrainttemplate.templates.gatekeeper.sh/k8sblockloadbalancer created
constrainttemplate.templates.gatekeeper.sh/k8sblocknodeport created
constrainttemplate.templates.gatekeeper.sh/k8sblockwildcardingress created
constrainttemplate.templates.gatekeeper.sh/k8scontainerlimits created
constrainttemplate.templates.gatekeeper.sh/k8scontainerrequests created
constrainttemplate.templates.gatekeeper.sh/k8sdisallowanonymous created
constrainttemplate.templates.gatekeeper.sh/k8sdisallowedrepos created
constrainttemplate.templates.gatekeeper.sh/k8sdisallowedtags created
constrainttemplate.templates.gatekeeper.sh/k8sdisallowinteractivetty created
constrainttemplate.templates.gatekeeper.sh/k8sexternalips created
constrainttemplate.templates.gatekeeper.sh/k8shttpsonly created
constrainttemplate.templates.gatekeeper.sh/k8simagedigests created
constrainttemplate.templates.gatekeeper.sh/k8spoddisruptionbudget created
constrainttemplate.templates.gatekeeper.sh/k8spspautomountserviceaccounttokenpod created
constrainttemplate.templates.gatekeeper.sh/k8srequiredannotations created
constrainttemplate.templates.gatekeeper.sh/k8srequiredlabels created
constrainttemplate.templates.gatekeeper.sh/k8srequiredresources created
constrainttemplate.templates.gatekeeper.sh/k8sstorageclass created
constrainttemplate.templates.gatekeeper.sh/k8suniqueingresshost created
constrainttemplate.templates.gatekeeper.sh/k8suniqueserviceselector created
```

### Apply Constraints

The 2nd part to OPA gatekeeper policy enforcment is the `Constraint`. The constraints
are a specific application of a template. This instructs the OPA gatekeeper to constrain
a resource according to specific parameters and applying one of the templates. For example,
a constraint can specify that every namespace must have a specific label applied.

Part of a constraint definition is the enforcement action. By default, OPA gatekeeper will enforce
constraints by denying the action. For the constraints here, we are using `dryrun`, which
does not deny the resources that violate constraints, but rather just logs the violation. This
allows us to see what resources are violating the constraints without the resources being denied.

Go to the opa library constraints directory.

```
cd engineering-journal-quantum-safe/opa/library/constraints
```

Apply the templates using `kustomize`

```
kustomize build . | kubectl apply -f -
```

Sample output

```
k8sallowedrepos.constraints.gatekeeper.sh/repo-is-amazonaws created
k8sallowedrepos.constraints.gatekeeper.sh/repo-is-openpolicyagent created
k8sblockendpointeditdefaultrole.constraints.gatekeeper.sh/block-endpoint-edit-default-role created
k8sblockloadbalancer.constraints.gatekeeper.sh/block-load-balancer created
k8sblocknodeport.constraints.gatekeeper.sh/block-node-port created
k8sblockwildcardingress.constraints.gatekeeper.sh/block-wildcard-ingress created
k8scontainerlimits.constraints.gatekeeper.sh/container-must-have-limits created
k8scontainerrequests.constraints.gatekeeper.sh/container-must-have-requests created
k8sdisallowanonymous.constraints.gatekeeper.sh/no-anonymous-no-authenticated created
k8sdisallowinteractivetty.constraints.gatekeeper.sh/no-interactive-tty-containers created
k8sdisallowedrepos.constraints.gatekeeper.sh/repo-must-not-be-k8s-gcr-io created
k8sdisallowedtags.constraints.gatekeeper.sh/container-image-must-not-have-latest-tag created
k8sexternalips.constraints.gatekeeper.sh/external-ips created
k8shttpsonly.constraints.gatekeeper.sh/ingress-https-only created
k8simagedigests.constraints.gatekeeper.sh/container-image-must-have-digest created
k8spspautomountserviceaccounttokenpod.constraints.gatekeeper.sh/psp-automount-serviceaccount-token-pod created
k8spoddisruptionbudget.constraints.gatekeeper.sh/pod-distruption-budget created
k8srequiredannotations.constraints.gatekeeper.sh/all-must-have-certain-set-of-annotations created
k8srequiredlabels.constraints.gatekeeper.sh/all-must-have-owner created
k8srequiredresources.constraints.gatekeeper.sh/container-must-have-limits-and-requests created
k8suniqueingresshost.constraints.gatekeeper.sh/unique-ingress-host created
```


### List Violations

Use the following command to list the violations.

```
kubectl get constraints
```

Sample output

```
NAME                                                                                     ENFORCEMENT-ACTION   TOTAL-VIOLATIONS
k8srequiredresources.constraints.gatekeeper.sh/container-must-have-limits-and-requests   dryrun               4

NAME                                                                 ENFORCEMENT-ACTION   TOTAL-VIOLATIONS
k8suniqueingresshost.constraints.gatekeeper.sh/unique-ingress-host   dryrun               0

NAME                                                                                ENFORCEMENT-ACTION   TOTAL-VIOLATIONS
k8sdisallowinteractivetty.constraints.gatekeeper.sh/no-interactive-tty-containers   dryrun               0

NAME                                                                          ENFORCEMENT-ACTION   TOTAL-VIOLATIONS
k8scontainerrequests.constraints.gatekeeper.sh/container-must-have-requests   dryrun               0

NAME                                                        ENFORCEMENT-ACTION   TOTAL-VIOLATIONS
k8shttpsonly.constraints.gatekeeper.sh/ingress-https-only   dryrun               0

NAME                                                                                         ENFORCEMENT-ACTION   TOTAL-VIOLATIONS
k8sblockendpointeditdefaultrole.constraints.gatekeeper.sh/block-endpoint-edit-default-role   dryrun               0

NAME                                                                       ENFORCEMENT-ACTION   TOTAL-VIOLATIONS
k8sdisallowedrepos.constraints.gatekeeper.sh/repo-must-not-be-k8s-gcr-io   dryrun               0

NAME                                                                                   ENFORCEMENT-ACTION   TOTAL-VIOLATIONS
k8sdisallowedtags.constraints.gatekeeper.sh/container-image-must-not-have-latest-tag   dryrun               0

NAME                                                                                        ENFORCEMENT-ACTION   TOTAL-VIOLATIONS
k8srequiredannotations.constraints.gatekeeper.sh/all-must-have-certain-set-of-annotations   dryrun               3

NAME                                                         ENFORCEMENT-ACTION   TOTAL-VIOLATIONS
k8sblocknodeport.constraints.gatekeeper.sh/block-node-port   dryrun               0
```

### Violation Details

To see the details of the violations, you can look at the constraint yaml. For example, to see the 
`container-must-have-limits-and-requests` constraint violations, run the following command.

```
kubectl get k8srequiredresources container-must-have-limits-and-requests -o yaml
```

Near the end of the yaml output, you will see violations listed.

```yaml
  violations:
  - enforcementAction: dryrun
    group: ""
    kind: Pod
    message: container <manager> does not have <{"cpu"}> limits defined
    name: gatekeeper-controller-manager-865cc64485-kgkkj
    namespace: gatekeeper-system
    version: v1
  - enforcementAction: dryrun
    group: ""
    kind: Pod
    message: container <manager> does not have <{"cpu"}> limits defined
    name: gatekeeper-controller-manager-865cc64485-h2x7d
    namespace: gatekeeper-system
    version: v1
  - enforcementAction: dryrun
    group: ""
    kind: Pod
    message: container <manager> does not have <{"cpu"}> limits defined
    name: gatekeeper-controller-manager-865cc64485-g4b72
    namespace: gatekeeper-system
    version: v1
  - enforcementAction: dryrun
    group: ""
    kind: Pod
    message: container <manager> does not have <{"cpu"}> limits defined
    name: gatekeeper-audit-c794d5f69-s2nmd
    namespace: gatekeeper-system
    version: v1
```

## Install the EBS driver to the cluster

We install the EBS CSI driver as this gives us access to GP3 block storage.

Download the example ebs iam policy

```tsx
curl -o iam-policy-example-ebs.json https://raw.githubusercontent.com/kubernetes-sigs/aws-ebs-csi-driver/master/docs/example-iam-policy.json
```

Create the policy. You can change  `AmazonEKS_EBS_CSI_Driver_Policy` to a different name, but if you do, make sure to change it in later steps too.

```tsx
aws iam create-policy \
--policy-name AmazonEKS_EBS_CSI_Driver_Policy \
--tags '{"Key": "Product","Value": "Guardium"}' \
--policy-document file://iam-policy-example-ebs.json
```
Output should be similar to below

```tsx
{
    "Policy": {
        "PolicyName": "AmazonEKS_EBS_CSI_Driver_Policy",
        "PolicyId": "ANPA24LVTCGN5YOUAVX2V",
        "Arn": "arn:aws:iam::<ACCOUNT ID>:policy/AmazonEKS_EBS_CSI_Driver_Policy",
        "Path": "/",
        "DefaultVersionId": "v1",
        "AttachmentCount": 0,
        "PermissionsBoundaryUsageCount": 0,
        "IsAttachable": true,
        "CreateDate": "2023-04-19T14:17:03+00:00",
        "UpdateDate": "2023-04-19T14:17:03+00:00",
        "Tags": [
            {
                "Key": "Product",
                "Value": "Guardium"
            }
        ]
    }
}


```

Let's export the returned arn as a env VAR for further use

```tsx
export ebs_driver_policy_arn=$(aws iam list-policies --query 'Policies[?PolicyName==`AmazonEKS_EBS_CSI_Driver_Policy`].Arn' --output text)
```

Now let's export the rolename we are going to create as a env var. We're going to append the cluster name to the role to differentiate in case we have multiple clusters in this account. You can share policies, but you cannot share roles.

```tsx
export ebs_driver_role_name=AmazonEKS_EBS_CSI_DriverRole-${clustername}
```

Create the service account
```tsx
eksctl create iamserviceaccount \
  --name ebs-csi-controller-sa \
  --namespace kube-system \
  --cluster ${clustername} \
  --attach-policy-arn $ebs_driver_policy_arn \
  --approve \
  --region=${region} \
  --tags "Product=Guardium" \
  --role-only \
  --role-name ${ebs_driver_role_name}
```

Let's export the created role arn as another env var
```tsx
export ebs_driver_role_arn=$(aws iam list-roles --query 'Roles[?RoleName==`AmazonEKS_EBS_CSI_DriverRole-'$clustername'`].Arn' --output text)
```

Create the addon for the cluster

```tsx
eksctl create addon \
--name aws-ebs-csi-driver \
--cluster ${clustername} \
--service-account-role-arn $ebs_driver_role_arn \
--region=${region} \
--force
```

Create the following StorageClass yaml to use gp3

```tsx
cat <<EOF |kubectl apply -f -
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ebs-gp3-sc
provisioner: ebs.csi.aws.com
parameters:
  type: gp3
  fsType: ext4
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
EOF
```

### Verifying EBS 

Run the following command

```tsx
kubectl apply -f - <<EOF 
apiVersion: v1 
kind: PersistentVolumeClaim 
metadata: 
  name: block-pvc 
spec: 
  storageClassName: ebs-gp3-sc
  accessModes: 
    - ReadWriteOnce 
  resources: 
    requests: 
      storage: 1Gi 
--- 
apiVersion: v1 
kind: Pod 
metadata: 
  name: mypod 
spec: 
  containers: 
    - name: myfrontend 
      image: nginx 
      volumeMounts: 
        - mountPath: "/var/www/html" 
          name: mypd 
  volumes: 
    - name: mypd 
      persistentVolumeClaim: 
        claimName: block-pvc 
EOF
```

Verify the PVC was created
```tsx
kubectl get pvc
NAME        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
block-pvc   Bound    pvc-67193212-3e10-46ab-a0a4-2834e3560c4a   1Gi        RWO            ebs-gp3-sc     <unset>                 7s
```

You should see the bound status of the above pvc. 

Delete the test pod and pvc

```tsx
kubectl delete -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: block-pvc
spec:
  storageClassName: ebs-gp3-sc
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
        - mountPath: "/var/www/html"
          name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: block-pvc
EOF
```

## Enable EFS on the cluster

By default when we create a cluster with eksctl it defines and installs `gp2` storage class which is backed by Amazon's EBS (elastic block storage). After that we installed the EBS CSI driver to support `gp3`. However, both `gp2` and `gp3` are both block storage. They will not support RWX in our cluster. We need to install an EFS storage class.

### Create IAM policy

Download the IAM policy document from GitHub. You can also view the [policy document](https://github.com/kubernetes-sigs/aws-efs-csi-driver/blob/master/docs/iam-policy-example.json)
        
```tsx
curl -o iam-policy-example-efs.json https://raw.githubusercontent.com/kubernetes-sigs/aws-efs-csi-driver/master/docs/iam-policy-example.json
```

Create the policy. You can change `AmazonEKS_EFS_CSI_Driver_Policy` to a different name, but if you do, make sure to change it in later steps too.

```tsx
aws iam create-policy \
--policy-name AmazonEKS_EFS_CSI_Driver_Policy \
--tags '{"Key": "Product","Value": "Guardium"}' \
--policy-document file://iam-policy-example-efs.json
```
Output should be similar to below
```tsx
{
    "Policy": {
        "PolicyName": "AmazonEKS_EFS_CSI_Driver_Policy",
        "PolicyId": "ANPA3WENOYSA6LSFRSZ6U",
        "Arn": "arn:aws:iam::803455550593:policy/AmazonEKS_EFS_CSI_Driver_Policy",
        "Path": "/",
        "DefaultVersionId": "v1",
        "AttachmentCount": 0,
        "PermissionsBoundaryUsageCount": 0,
        "IsAttachable": true,
        "CreateDate": "2024-08-23T19:57:13+00:00",
        "UpdateDate": "2024-08-23T19:57:13+00:00",
        "Tags": [
            {
                "Key": "Product",
                "Value": "Guardium"
            }
        ]
    }
}
```

Let's export that policy arn as another env var
```tsx
export efs_driver_policy_arn=$(aws iam list-policies --query 'Policies[?PolicyName==`AmazonEKS_EFS_CSI_Driver_Policy`].Arn' --output text)
```
### Create IAM role

Now let’s export the rolename we are going to create as a env var. We’re going to append the cluster name to the role to differentiate in case we have multiple clusters in this account. You can share policies, but you cannot share roles.

```tsx
export efs_driver_role_name=AmazonEKS_EFS_CSI_DriverRole-${clustername}
```

Create an IAM role and attach the IAM policy to it. Annotate the Kubernetes service account with the IAM role ARN and the IAM role with the Kubernetes service account name. 

```tsx
eksctl create iamserviceaccount \
    --cluster ${clustername} \
    --namespace kube-system \
    --name efs-csi-controller-sa \
    --role-name ${efs_driver_role_name} \
    --attach-policy-arn $efs_driver_policy_arn \
    --tags "Product=Guardium" \
    --approve \
    --region ${region}
```

Once created, check the iam service account is created running the following command.

```bash
eksctl get iamserviceaccount --cluster ${clustername} --region ${region}

```
Should return
```tsx
NAMESPACE	NAME			ROLE ARN
kube-system	ebs-csi-controller-sa	arn:aws:iam::803455550593:role/AmazonEKS_EBS_CSI_DriverRole
kube-system	efs-csi-controller-sa	arn:aws:iam::803455550593:role/AmazonEKS_EFS_CSI_DriverRole
```

### Install EFS CSI driver

Now we just need our add-on registry address. This can be found here: https://docs.aws.amazon.com/eks/latest/userguide/add-ons-images.html

:::note
The add-on registry address is per region. So based on the URL above, since our region is `us-east-1`, then our registry address would be `602401143452.dkr.ecr.us-east-1.amazonaws.com/eks/aws-efs-csi-driver`
:::

Let's install the driver add-on to our clusters. We're going to use `helm` for this.

```tsx
helm repo add aws-efs-csi-driver https://kubernetes-sigs.github.io/aws-efs-csi-driver/

helm repo update
```

Install a release of the driver using the Helm chart. Replace the repository address with the cluster's [container image address](https://docs.aws.amazon.com/eks/latest/userguide/add-ons-images.html).

```tsx
helm upgrade -i aws-efs-csi-driver aws-efs-csi-driver/aws-efs-csi-driver \
    --namespace kube-system \
    --set image.repository=602401143452.dkr.ecr.us-east-1.amazonaws.com/eks/aws-efs-csi-driver \
    --set controller.serviceAccount.create=false \
    --set controller.serviceAccount.name=efs-csi-controller-sa

```

Verify that it installed correctly with this command
```tsx
kubectl get pod -n kube-system -l "app.kubernetes.io/name=aws-efs-csi-driver,app.kubernetes.io/instance=aws-efs-csi-driver"
```

Should return something like
```tsx
NAME                                  READY   STATUS    RESTARTS   AGE
efs-csi-controller-7fc77768fc-2swkw   3/3     Running   0          2m13s
efs-csi-controller-7fc77768fc-c69rb   3/3     Running   0          2m13s
efs-csi-node-ccxns                    3/3     Running   0          2d17h
efs-csi-node-k52pt                    3/3     Running   0          2d17h
efs-csi-node-r8nbm                    3/3     Running   0          2d17h
```

### Create the EFS Filesystem
Now we need to create the filesystem in EFS so we can use it

Export the following variables. 

Get our VPC ID
```tsx
vpc_id=$(aws eks describe-cluster \
    --name $clustername \
    --query "cluster.resourcesVpcConfig.vpcId" \
    --region $region \
    --output text)
```
Retrieve the CIDR range for your cluster's VPC and store it in a variable for use in a later step.

```tsx
cidr_range=$(aws ec2 describe-vpcs \
    --vpc-ids $vpc_id \
    --query "Vpcs[].CidrBlock" \
    --output text \
    --region $region)
```
Create a security group with an inbound rule that allows inbound NFS traffic for your Amazon EFS mount points.

```tsx
security_group_id=$(aws ec2 create-security-group \
    --group-name EFS4FileNetSecurityGroup \
    --description "EFS security group for Guardium Insight Clusters" \
    --vpc-id $vpc_id \
    --region $region \
    --output text)
```

Create an inbound rule that allows inbound NFS traffic from the CIDR for your cluster's VPC.

```tsx
aws ec2 authorize-security-group-ingress \
    --group-id $security_group_id \
    --protocol tcp \
    --port 2049 \
    --region $region \
    --cidr $cidr_range
```

Create a file system.
```tsx
file_system_id=$(aws efs create-file-system \
    --region $region \
    --encrypted \
    --tags '{"Key": "Product","Value": "Guardium"}' \
    --performance-mode generalPurpose \
    --query 'FileSystemId' \
    --output text)
```

Create mount targets.

Determine the IDs of the subnets in your VPC and which Availability Zone the subnet is in.
```tsx
aws ec2 describe-subnets \
    --filters "Name=vpc-id,Values=$vpc_id" \
    --query 'Subnets[*].{SubnetId: SubnetId,AvailabilityZone: AvailabilityZone,CidrBlock: CidrBlock}' \
    --region $region \
    --output table
```
Should output something like the following
```tsx
----------------------------------------------------------------------
|                           DescribeSubnets                          |
+------------------+--------------------+----------------------------+
| AvailabilityZone |     CidrBlock      |         SubnetId           |
+------------------+--------------------+----------------------------+
|  us-east-1c      |  192.168.64.0/19   |  subnet-08c33dce5e63c82dc  |
|  us-east-1b      |  192.168.32.0/19   |  subnet-0f7a2b449320cc1e6  |
|  us-east-1a      |  192.168.0.0/19    |  subnet-0ec499ae3eae19eb0  |
|  us-east-1b      |  192.168.128.0/19  |  subnet-04f3d465138687333  |
|  us-east-1a      |  192.168.96.0/19   |  subnet-0bc4d31344c60c113  |
|  us-east-1c      |  192.168.160.0/19  |  subnet-0bee6fc06187cafd1  |
+------------------+--------------------+----------------------------+
```

Add mount targets for the subnets that your nodes are in.

Run the following command:
```bash
for subnet in $(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$vpc_id" --query 'Subnets[*].{SubnetId: SubnetId,AvailabilityZone: AvailabilityZone,CidrBlock: CidrBlock}' --region $region --output text | awk '{print $3}') ; do aws efs create-mount-target --file-system-id $file_system_id --region $region --subnet-id $subnet --security-groups $security_group_id ; done
```

This wraps the below command in a for loop that will iterate through your subnet ids.
```tsx
aws efs create-mount-target \
    --file-system-id $file_system_id \
    --region $region \
    --subnet-id <SUBNETID> \
    --security-groups $security_group_id
```

### Create EFS Storage Class
Create a storage class for dynamic provisioning

Let's get our filesystem ID if we don't already have it above. However if you ran the above steps, `$file_system_id` should already be defined.
```tsx
aws efs describe-file-systems \
--query "FileSystems[*].FileSystemId" \
--region $region \
--output text

fs-071439ffb7e10b67b
```

{/*
Download a `StorageClass` manifest for Amazon EFS.
```tsx
curl -o EFSStorageClass.yaml https://raw.githubusercontent.com/kubernetes-sigs/aws-efs-csi-driver/master/examples/kubernetes/dynamic_provisioning/specs/storageclass.yaml

```
*/}

:::note[Super Important!]

If you did not export the `$file_system_id` variable, make sure the filesystem id you use in the below command is the filesystem id that was returned to you above!

{/*
You can retrieve the file system id with the following command:

```tsx

```
*/}

Also if you are going to use EFS as your default storage, follow the instructions below. Otherwise just create the `efs-sc` storage class and skip the `efs-sc-pg` storage class as we will set the default to the gp3 block storage.

:::

Create the storage class

```tsx
cat <<EOF | envsubst | kubectl apply -f -
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: efs-sc
parameters:
  uid: "0"
  gid: "0"
  directoryPerms: "777"
  fileSystemId: ${file_system_id}
  provisioningMode: efs-ap
provisioner: efs.csi.aws.com
reclaimPolicy: Delete
volumeBindingMode: Immediate
EOF
```

Create this second storageclasses for EFS for common-services operator that will be installed as part of IBM Foundational Services. You will only need this second storage class if you intend on using EFS instead of block storage.

```tsx
cat <<EOF | envsubst | kubectl apply -f -
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: efs-sc-pg
parameters:
  uid: "26"
  gid: "65534"
  directoryPerms: "777"
  fileSystemId: ${file_system_id}
  provisioningMode: efs-ap
provisioner: efs.csi.aws.com
reclaimPolicy: Delete
volumeBindingMode: Immediate
EOF
```

{/*
Update it with the storage class id as well as the following vars
```bash
sed -i "s/fileSystemId:.*\/fileSystemId: $file_system_id/" EFSStorageClass.yaml
sed -i "s/directoryPerms:.*\/directoryPerms: \"777\"/" EFSStorageClass.yaml
sed -i "/basePath:.*\/d" EFSStorageClass.yaml
sed -i "/parameters:/a\  gid: \"0\"\n  uid: \"0\"" EFSStorageClass.yaml
```
*/}

{/*
If using the default `sed` command that comes with MacOS


Deploy the storage class.

```bash
kubectl apply -f EFSStorageClass.yaml
```
*/}


:::note[Setting Default Storage Class]
Set one of the EFS storage classes as the default storage class only if you intend on primarily using EFS. Otherwise set block storage as default.

If using EFS as primary storage
```bash
kubectl patch storageclass efs-sc -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
```

If using EBS(block) as primary storage
```bash
kubectl patch storageclass ebs-gp3-sc -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
```
:::


Finally, verify they are both there
```tsx
kubectl get sc
NAME               PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
ebs-gp3-sc         ebs.csi.aws.com         Delete          WaitForFirstConsumer   false                  6d18h
efs-sc (default)   efs.csi.aws.com         Delete          Immediate              false                  16h
efs-sc-pg          efs.csi.aws.com         Delete          Immediate              false                  14h
gp2                kubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   false                  6d23h
```

### Verify EFS

Run the following command to create a pod and a PVC using the default EFS storage class

```tsx
kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: block-pvc
spec:
  storageClassName: efs-sc
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
        - mountPath: "/var/www/html"
          name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: block-pvc
EOF
```

Running the following command should verify the PVC was successfully created and bound
```tsx
kubectl get pvc
```

```tsx[OUTPUT]
NAME        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
block-pvc   Bound    pvc-dc711246-02a1-4a9d-b428-a2476b17dd8c   1Gi        RWO            efs-sc         <unset>                 4s
```

Now we can delete our test pod and pvc
```tsx
kubectl delete -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: block-pvc
spec:
  storageClassName: efs-sc
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
        - mountPath: "/var/www/html"
          name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: block-pvc
EOF
```

## Install NGINX Controller

:::note[On Ingresses]

If you plan on using AWS ALB for ingress, follow the directions [here](../04_optional_services/#optional-install-the-alb-ingress-controller)

:::
Let's install the NGINX helm chart
```tsx
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm repo update
```

{/*
Modify the NGINX controller deployment file

Pull down the NGINX controller deployment
```
wget -O nginx-deploy.yaml https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/aws/deploy.yaml
```

Export our relevant subnet ids to a comma delimited env variable

```tsx
export subnets=$(aws eks describe-cluster --name ${clustername} --region ${region} --query "cluster.resourcesVpcConfig.subnetIds" --output json | jq -r 'join(",")' | sed s/,/\\\\,/g)
```
*/}

Create the namespace for NGINX

```tsx
kubectl create ns ingress-nginx
```

Now install the helm chart for nginx

{/*--set-string controller.service.annotations.'service\.beta\.kubernetes\.io/aws-load-balancer-subnets'="$subnets" \ */}

```tsx
helm install ingress-nginx ingress-nginx/ingress-nginx \
--set-string controller.service.annotations.'service\.beta\.kubernetes\.io/aws-load-balancer-backend-protocol'=tcp \
--set-string controller.service.annotations.'service\.beta\.kubernetes\.io/aws-load-balancer-cross-zone-load-balancing-enabled'="true" \
--set-string controller.service.annotations.'service\.beta\.kubernetes\.io/aws-load-balancer-type'=nlb \
--namespace=ingress-nginx

```

Run the following command to verify that a load balancer was assigned

```tsx
kubectl get service --namespace ingress-nginx ingress-nginx-controller
```

```tsx[OUTPUT]
NAME                       TYPE           CLUSTER-IP       EXTERNAL-IP                                                                     PORT(S)                      AGE
ingress-nginx-controller   LoadBalancer   10.100.204.215   a904f47d90a8c4961ae389b155359368-aa0f265b8588c710.elb.us-east-1.amazonaws.com   80:30870/TCP,443:31883/TCP   61s
```

{/*
```tsx
sed "/    service.beta.kubernetes.io\/aws-load-balancer-type: nlb/a\    service.beta.kubernetes.io\/aws-load-balancer-subnets: \"$subnets\"" nginx-deploy.yaml
```

```zsh

```

Modify the deployment file by adding the annotations that do not exist and editing the one(s) that do:
```tsx
service.beta.kubernetes.io/aws-load-balancer-type: "external" // set this to "internal" if not using a public ip
service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: "instance"
service.beta.kubernetes.io/aws-load-balancer-scheme: "internet-facing"
```

Also search for the following configmap entry in the deployment file:

```tsx
---
apiVersion: v1
data:
  allow-snippet-annotations: "false"
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.9.1
  name: ingress-nginx-controller
  namespace: ingress-nginx
---
```
We want to add the following annotations:

```tsx
allow-snippet-annotations: "true"
enable-underscores-in-headers: "true"
nginx.ingress.kubernetes.io/proxy-body-size: "0"
```
So now our config map should look like this
```tsx
---
apiVersion: v1
data:
  allow-snippet-annotations: "false"
kind: ConfigMap
metadata:
  annotations:
    allow-snippet-annotations: "true"
    enable-underscores-in-headers: "true"
    nginx.ingress.kubernetes.io/proxy-body-size: "0" 
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.9.1
  name: ingress-nginx-controller
  namespace: ingress-nginx
---
```

Now apply the deployment
```bash
kubectl apply -f nginx-deploy.yaml
```
*/}

### Verify the NGINX deployment

Verify the deployment

Command:

```bash
kubectl get ingressclass
```
Example output:

```tsx
NAME    CONTROLLER             PARAMETERS   AGE
nginx   k8s.io/ingress-nginx   <none>       2m43s
```
{/*
Once created, check the iam service account is created running the following command.

```bash
eksctl get iamserviceaccount --cluster <cluster-name>

NAMESPACE	NAME				ROLE ARN
kube-system	aws-load-balancer-controller	arn:aws:iam::748107796891:role/AmazonEKSLoadBalancerControllerRole
kube-system	efs-csi-controller-sa		arn:aws:iam::748107796891:role/eksctl-filenet-cluster-east-addon-iamserviceaccount-ku-Role1-1SCBRU1DS52QY
```
*/}

## Install the Operator Lifecycle Manager

Run the following command using the `operator-sdk`

```tsx
operator-sdk olm install
```

Verify the installation

```tsx
kubectl get csv -n olm | grep packageserver
```

```tsx[OUTPUT]
packageserver   Package Server   0.28.0               Succeeded
```

## Create the required Namespace

```tsx
kubectl create ns openshift-marketplace
```

Set our context to that namespace and export it as an env var

```tsx
export NAMESPACE=openshift-marketplace

kubectl config set-context --current --namespace $NAMESPACE
```

## Install the IBM Cert Manager

If you followed the directions [here](../01_pre-reqs/#downloading-the-case-file) you should have the `ibm-guardium-insights` case file downloaded and extracted.

Change to the following directory
```tsx
cd ibm-guardium-insights/inventory/install/files/support/eks
```

Create the namespace and then run the installation script

```tsx
kubectl create namespace ibm-cert-manager
chmod +x ibm-cert-manager.sh
./ibm-cert-manager.sh
```

{/*If you need to limit the cert manager to a single namespace, edit the `ibm-cert-manager.sh` script.*/}

Give it a few minutes and then verify the cert manager is up

```tsx
kubectl get po -n ibm-cert-manager
NAME                                                              READY   STATUS      RESTARTS   AGE
cd6e1c2b84458a4f431f49f499a919d28af0b23693e36f2fc53bc1f2c3hw5pw   0/1     Completed   0          2m1s
cert-manager-cainjector-85777f77cc-hbzg7                          1/1     Running     0          102s
cert-manager-controller-957bc947-kg5wx                            1/1     Running     0          102s
cert-manager-webhook-5586d798f-lcsln                              1/1     Running     0          102s
ibm-cert-manager-catalog-rcbvc                                    1/1     Running     0          2m12s
ibm-cert-manager-operator-64fc5b4644-rnpqg                        1/1     Running     0          108s
```

```tsx
kubectl get csv -n ibm-cert-manager
NAME                               DISPLAY            VERSION   REPLACES   PHASE
ibm-cert-manager-operator.v4.2.7   IBM Cert Manager   4.2.7                Succeeded
```

:::note[Deploying with ACM]
If you plan to use AWS ACM to manage your certs with an ALB ingress, follow the install instructions [here](../04_optional_services/#use-aws-ca-acm-for-certs-optional)
:::

## Install Foundational Services

:::note[Entitlement]
As noted [here](../guardium_insights/01_pre-reqs/) make sure you have retrieved your entitlement key and applied it to the namespace

Export your entitlement key as an env var

```tsx
export IBMKEY="<entitlement key>"
```

Set the entitlement key

```tsx
kubectl create secret docker-registry ibm-entitlement-key \
    --docker-username=cp \
    --docker-password=${IBMKEY} \
    --docker-server=cp.icr.io \
    --namespace=${NAMESPACE}
```

:::


Change to the following directory
```tsx
cd ibm-guardium-insights/inventory/install/files/support/eks
```

Export the following vars

```tsx
export REPLACE_NAMESPACE=openshift-marketplace
export NAMESPACE=openshift-marketplace
```

{/*We're going to use the loadbalancer name for this as we aren't using any DNS (hope it works)*/}

For our example setup, we are working with the `thinkforward.work` domain. So we set the domain in our remote dns to `apps.gi.thinkforward.work`. Other users may need to configure their FQDN in AWS Route 53.

```tsx
export HOSTNAME=apps.gi.thinkforward.work
```

:::note[On Storageclasses]

Make the following modifications to the `CPFS-install-template.sh` ONLY if you are using the custom EFS storage class instead of block storage.

Open the CPFS-install-template.sh in an editor and locate the following code block

```tsx
operand_request=$(cat <<EOF
apiVersion: operator.ibm.com/v1alpha1
kind: OperandRequest
metadata:
  name: common-service
  namespace: $NAMESPACE
spec:
  requests:
    - operands:
        - name: ibm-im-operator
        - name: ibm-platformui-operator
        - name: cloud-native-postgresql
        - name: ibm-events-operator
      registry: common-service
      registryNamespace: $NAMESPACE
EOF
)
```

Insert the following line just before that code block

```tsx
sleep 30
kubectl patch --type=merge commonservice common-service -p '{"spec": {"storageClass":"efs-sc-pg"}}'
```

Now it should look like the following

```tsx
sleep 30
kubectl patch --type=merge commonservice common-service -p '{"spec": {"storageClass":"efs-sc-pg"}}'

operand_request=$(cat <<EOF
apiVersion: operator.ibm.com/v1alpha1
kind: OperandRequest
metadata:
  name: common-service
  namespace: $NAMESPACE
spec:
  requests:
    - operands:
        - name: ibm-im-operator
        - name: ibm-platformui-operator
        - name: cloud-native-postgresql
        - name: ibm-events-operator
      registry: common-service
      registryNamespace: $NAMESPACE
EOF
)
```
We're adding the `sleep 30` so the common-services CR will become available
:::
{/*Mental note here, Delete everything and test without creating the operands so we can make sure that the CR is actually created prior to the operand installation*/}

### Run the CPFS install script

```tsx
chmod +x CPFS-install-template.sh
./CPFS-install-template.sh
```

If you see the following error out, re-run the command
```tsx {10-11}
No resources found in openshift-marketplace namespace.
operatorgroup.operators.coreos.com/operatorgroup created
configmap/ibm-cpp-config created
catalogsource.operators.coreos.com/opencloud-operators created
catalogsource.operators.coreos.com/cloud-native-postgresql-catalog created
subscription.operators.coreos.com/ibm-common-service-operator created
Validate the installation
Waiting for CRD commonservices.operator.ibm.com to be created in namespace openshift-marketplace... Attempt 1/5
CRD commonservices.operator.ibm.com exists.
error: resource mapping not found for name: "common-service" namespace: "openshift-marketplace" from "STDIN": no matches for kind "OperandRequest" in version "operator.ibm.com/v1alpha1"
ensure CRDs are installed first
```

Re-run should return
```tsx
OperatorGroup already exists in namespace openshift-marketplace.
configmap/ibm-cpp-config configured
catalogsource.operators.coreos.com/opencloud-operators unchanged
catalogsource.operators.coreos.com/cloud-native-postgresql-catalog unchanged
subscription.operators.coreos.com/ibm-common-service-operator unchanged
Validate the installation
CRD commonservices.operator.ibm.com exists.
operandrequest.operator.ibm.com/common-service unchanged
```

{/*
### CURRENT FAILURES

A complete install of the ibm-common-services shows the following failure:

```tsx
create-postgres-license-config-7f88z                              0/1     Init:ImagePullBackOff   0          40s
```

Digging into the logs shows this as entitled software. This means it needs an IBM entitlement key.
```tsx
Events:
  Type     Reason                           Age                From               Message
  ----     ------                           ----               ----               -------
  Normal   Scheduled                        57s                default-scheduler  Successfully assigned openshift-marketplace/create-postgres-license-config-7f88z to ip-192-168-17-40.ec2.internal
  Normal   Pulling                          17s (x3 over 57s)  kubelet            Pulling image "cp.icr.io/cp/cpd/edb-postgres-license-provider@sha256:c1670e7dd93c1e65a6659ece644e44aa5c2150809ac1089e2fd6be37dceae4ce"
  Warning  Failed                           17s (x3 over 56s)  kubelet            Failed to pull image "cp.icr.io/cp/cpd/edb-postgres-license-provider@sha256:c1670e7dd93c1e65a6659ece644e44aa5c2150809ac1089e2fd6be37dceae4ce": failed to pull and unpack image "cp.icr.io/cp/cpd/edb-postgres-license-provider@sha256:c1670e7dd93c1e65a6659ece644e44aa5c2150809ac1089e2fd6be37dceae4ce": failed to resolve reference "cp.icr.io/cp/cpd/edb-postgres-license-provider@sha256:c1670e7dd93c1e65a6659ece644e44aa5c2150809ac1089e2fd6be37dceae4ce": failed to authorize: failed to fetch anonymous token: unexpected status from GET request to https://cp.icr.io/oauth/token?scope=repository%3Acp%2Fcpd%2Fedb-postgres-license-provider%3Apull&service=registry: 400 Bad Request
  Warning  Failed                           17s (x3 over 56s)  kubelet            Error: ErrImagePull
  Warning  FailedToRetrieveImagePullSecret  2s (x6 over 57s)   kubelet            Unable to retrieve some image pull secrets (ibm-entitlement-key); attempting to pull the image may not succeed.
  Normal   BackOff                          2s (x3 over 55s)   kubelet            Back-off pulling image "cp.icr.io/cp/cpd/edb-postgres-license-provider@sha256:c1670e7dd93c1e65a6659ece644e44aa5c2150809ac1089e2fd6be37dceae4ce"
  Warning  Failed                           2s (x3 over 55s)   kubelet            Error: ImagePullBackOff
```

Also

```tsx
common-service-db-1-initdb-tmpwc                                  0/1     Pending                 0          2m36s
```

This appears to be hanging on an EFS volume being provisioned. Not sure why this is happening.

```tsx
kubectl get pvc
NAME                      STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
common-service-db-1       Bound     pvc-adb72a84-4a21-4b6b-91bd-3b3969ea3a99   10Gi       RWO            efs-sc         <unset>                 2m50s
common-service-db-1-wal   Pending                                                                        efs-sc         <unset>                 2m50s
```

It seems the path name was too long. We adjusted it by removing the path directive from the storage class
*/}


{/*

Getting the uid/gid for postgres (for the storage class)

```tsx
kubectl describe clusters.postgresql.k8s.enterprisedb.io common-service-db | egrep "UID|GID"
```

Should return a bunch of stuff, but we want this

```tsx
  UID:                 1cd7dd91-bf67-4fbe-8967-44323dad3b55
  Postgres GID:               26
  Postgres UID:               26
```

So questions....

- How do we specify the storage class for this instead of depending on whatever is set to default?

We updated the efs-sc storage class by deleting it, adding the uid of 26 and setting gid to 65534 and reapplying it.

Now we are testing the deployment again.

BANG IT WORKS

Things to check on

Updating the common-service-db CR to use a specific storage class

*/}

### Verify Foundational Services are properly installed

```tsx
kubectl get csv | grep ibm
```

Output should look like this:

```tsx
ibm-cert-manager-operator.v4.2.7              IBM Cert Manager                       4.2.7                                        Succeeded
ibm-common-service-operator.v4.6.2            IBM Cloud Pak foundational services    4.6.2                                        Succeeded
ibm-commonui-operator.v4.4.1                  Ibm Common UI                          4.4.1                                        Succeeded
ibm-events-operator.v5.0.1                    IBM Events Operator                    5.0.1                                        Succeeded
ibm-iam-operator.v4.5.1                       IBM IM Operator                        4.5.1                                        Succeeded
ibm-zen-operator.v5.1.4                       IBM Zen Service                        5.1.4                                        Succeeded
```

### Verify that the operand requests have completed and installed

```tsx
kubectl get opreq
```

Output should look like this:

```tsx
NAME                          AGE   PHASE     CREATED AT
common-service                97m   Running   2024-08-30T13:45:01Z
ibm-iam-request               97m   Running   2024-08-30T13:45:29Z
postgresql-operator-request   96m   Running   2024-08-30T13:45:58Z
```

### Patch the common-service CR to accept the license

```tsx
kubectl patch commonservices common-service --type merge -p '{"spec":{"license":{"accept":true}}}'
```

## Install network policies for Foundational Services

```tsx
export CS_NAMESPACE=openshift-marketplace
cp -R ibm-guardium-insights/inventory/ibmCommonServiceOperatorSetup/files/cp3-networkpolicy/ingress ibm-guardium-insights/inventory/install/files/support/eks/
./install_networkpolicy.sh -n $CS_NAMESPACE
```

Verify the policies have all been created

```tsx
kubectl get netpol
```

Output should look like this:

```tsx
NAME                                     POD-SELECTOR                                     AGE
access-to-audit-svc                      component=zen-audit                              86m
access-to-common-web-ui                  k8s-app=common-web-ui                            87m
access-to-edb-postgres                   k8s.enterprisedb.io/cluster                      86m
access-to-edb-postgres-webhooks          app.kubernetes.io/name=cloud-native-postgresql   86m
access-to-ibm-common-service-operator    name=ibm-common-service-operator                 86m
access-to-ibm-nginx                      component=ibm-nginx                              86m
access-to-icp-mongodb                    app=icp-mongodb                                  86m
access-to-platform-auth-service          k8s-app=platform-auth-service                    86m
access-to-platform-identity-management   k8s-app=platform-identity-management             86m
access-to-platform-identity-provider     k8s-app=platform-identity-provider               86m
access-to-usermgmt                       component=usermgmt                               86m
access-to-volumes                        icpdsupport/app=volumes                          86m
access-to-zen-core                       component=zen-core                               86m
access-to-zen-core-api                   component=zen-core-api                           86m
access-to-zen-meta-api                   app.kubernetes.io/instance=ibm-zen-meta-api      86m
access-to-zen-minio                      component=zen-minio                              86m
access-to-zen-watchdog                   component=zen-watchdog                           86m
allow-iam-config-job                     component=iam-config-job                         86m
allow-webhook-access-from-apiserver      <none>                                           86m
```

## Install Kubernetes Ingresses

Export the following values. As above, the example FQDN we are using is `apps.gi.thinkforward.work`. Yours may be different depending on what you've set in your DNS or Route 53.
```tsx
export REPLACE_NAMESPACE=openshift-marketplace
export HOSTNAME=apps.gi.thinkforward.work
```

Run the Configmap creation with the following commands

```tsx
while [[ $(kubectl get secret platform-oidc-credentials -ojsonpath='{.data.WLP_CLIENT_SECRET}' | base64 -d | wc -c) -eq 0 ]] ; do echo Waiting 30 seconds for platform-oidc-credentials to be created with the correct content; sleep 30; done
client_id="$(kubectl get secret platform-oidc-credentials -o yaml |grep 'WLP_CLIENT_ID:' | sed 's/WLP_CLIENT_ID: *//')"
client_id=`echo $client_id|base64 --decode`
echo $client_id


cat cpfs-ingress-template.yaml | sed 's#REPLACE_CLIENT_ID#'$client_id'#g' | sed 's#REPLACE_NAMESPACE#'$NAMESPACE'#g' | sed 's#REPLACE_HOSTNAME#'$HOSTNAME'#g' | kubectl apply -f -
```

Verify the ingress creation

```tsx
kubectl get ingres
```

Output should look like this:

```tsx
NAME                         CLASS   HOSTS                                                         ADDRESS   PORTS   AGE
cncf-common-web-ui           nginx   cp-console-openshift-marketplace.apps.gi.thinkforward.work             80      13s
cncf-id-mgmt                 nginx   cp-console-openshift-marketplace.apps.gi.thinkforward.work             80      13s
cncf-platform-auth           nginx   cp-console-openshift-marketplace.apps.gi.thinkforward.work             80      13s
cncf-platform-id-auth        nginx   cp-console-openshift-marketplace.apps.gi.thinkforward.work             80      13s
cncf-platform-id-provider    nginx   cp-console-openshift-marketplace.apps.gi.thinkforward.work             80      13s
cncf-platform-login          nginx   cp-console-openshift-marketplace.apps.gi.thinkforward.work             80      13s
cncf-platform-oidc           nginx   cp-console-openshift-marketplace.apps.gi.thinkforward.work             80      13s
cncf-saml-ui-callback        nginx   cp-console-openshift-marketplace.apps.gi.thinkforward.work             80      13s
cncf-social-login-callback   nginx   cp-console-openshift-marketplace.apps.gi.thinkforward.work             80      13s
```

### Verify Ingress

Verify that the common webui is now up and available by going to the link above (this will reflect whatever you set for your domain). In our case it is `cp-console-openshift-marketplace.apps.gi.thinkforward.work`

![Webui](../../../assets/images/webui01.png)