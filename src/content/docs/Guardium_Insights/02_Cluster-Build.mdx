---
title: EKS Cluster build
description: Running Guardium Insights with QSPM on EKS
sidebar:
  label: Cluster Build
  order: 2
---

:::note[Useful Links]

Some of these steps come from this documentation for [Guardium Insights on EKS](https://www.ibm.com/docs/en/guardium-insights/3.x?topic=scenarios-installation-amazon-elastic-kubernetes-service-eks)

:::

## Cluster Creation

### Deploying a cluster with `eksctl`

:::note
This assumes you have already configured `eksctl` and your aws configuration with your account info.

This also assumes you will be creating the cluster in the us-east region. This can be set to whatever region you want.
:::

Run the `eksctl` command below to create your first cluster and perform the following:

-   Create a 3-node Kubernetes cluster named `gi-east` with node type as `m6i.4xlarge` and region as `us-east-1`.
-   Control plane is managed by AWS EKS, so sizing for control plane nodes doesn't apply.
-   Define a minimum of one node (`--nodes-min 1`) and a maximum of three-node (`--nodes-max 3`) for this node group managed by EKS.
-   Create a node group with the name `guardium-workers` and select a machine type for the `guardium-workers` node group.

:::caution[Multiple AZ support]
As of this writing, GI v3.4.0 does not support multiple availability zones. 
:::

Let's set some env vars to make our lives easier

```bash
export clustername=gi-east
export region=us-east-1
```

```tsx
eksctl create cluster \
--name ${clustername} \
--version 1.30 \
--region ${region} \
--zones ${region}a,${region}b \
--nodegroup-name guardium-workers \
--node-type m6i.4xlarge \
--with-oidc \
--nodes 3 \
--nodes-min 1 \
--nodes-max 3 \
--node-zones ${region}a \
--tags "Product=Guardium" \
--managed
```
### OIDC
Associate an IAM oidc provider with the cluster if you didn't include `--with-oidc` above.
```tsx
eksctl utils associate-iam-oidc-provider --region=${region} --cluster=${clustername} --approve
```
### Configure `kubectl`

Once the cluster is up, add it to your kube config. `eksctl` will probably do this for you.

```tsx
aws eks update-kubeconfig --name ${clustername} --region ${region}
```

{/*
### Create the EKS Pod Identity Agent Addon

We're going to use the EKS pod identity management addon to as IRSA (IAM Roles for Service Accounts) is deprecated and Pod Identity Management requires less permissions for a user who may not have IAM privileges.

```tsx
eksctl create addon \
--name eks-pod-identity-agent \
--cluster ${clustername} \
--region ${region} \
--version latest
```

Let's verify the addon was created

```tsx
eksctl get addon --cluster ${clustername} --region ${region} --name eks-pod-identity-agent -o json 
```

Should return something like this
```tsx
2024-08-23 12:09:50 [ℹ]  Kubernetes version "1.30" in use by cluster "gi-east"
2024-08-23 12:09:51 [ℹ]  to see issues for an addon run `eksctl get addon --name <addon-name> --cluster <cluster-name>`
[
    {
        "Name": "eks-pod-identity-agent",
        "Version": "v1.3.0-eksbuild.1",
        "NewerVersion": "",
        "IAMRole": "",
        "Status": "ACTIVE",
        "ConfigurationValues": "",
        "Issues": null,
        "PodIdentityAssociations": null
    }
]
```
*/}

### Install the EKS helm repo

```tsx
helm repo add eks https://aws.github.io/eks-charts
helm repo update
```

### Install the EBS driver to the cluster

We install the EBS CSI driver as this gives us access to GP3 block storage.

Download the example ebs iam policy

```tsx
curl -o iam-policy-example-ebs.json https://raw.githubusercontent.com/kubernetes-sigs/aws-ebs-csi-driver/master/docs/example-iam-policy.json
```

Create the policy. You can change  `AmazonEKS_EBS_CSI_Driver_Policy` to a different name, but if you do, make sure to change it in later steps too.

```tsx
aws iam create-policy \
--policy-name AmazonEKS_EBS_CSI_Driver_Policy \
--tags '{"Key": "Product","Value": "Guardium"}' \
--policy-document file://iam-policy-example-ebs.json
```
Output should be similar to below

```tsx
{
    "Policy": {
        "PolicyName": "AmazonEKS_EBS_CSI_Driver_Policy",
        "PolicyId": "ANPA24LVTCGN5YOUAVX2V",
        "Arn": "arn:aws:iam::<ACCOUNT ID>:policy/AmazonEKS_EBS_CSI_Driver_Policy",
        "Path": "/",
        "DefaultVersionId": "v1",
        "AttachmentCount": 0,
        "PermissionsBoundaryUsageCount": 0,
        "IsAttachable": true,
        "CreateDate": "2023-04-19T14:17:03+00:00",
        "UpdateDate": "2023-04-19T14:17:03+00:00",
        "Tags": [
            {
                "Key": "Product",
                "Value": "Guardium"
            }
        ]
    }
}


```

Let's export the returned arn as a env VAR for further use

```tsx
export ebs_driver_policy_arn=$(aws iam list-policies --query 'Policies[?PolicyName==`AmazonEKS_EBS_CSI_Driver_Policy`].Arn' --output text)
```

Create the service account
```tsx
eksctl create iamserviceaccount \
  --name ebs-csi-controller-sa \
  --namespace kube-system \
  --cluster ${clustername} \
  --attach-policy-arn $ebs_driver_policy_arn \
  --approve \
  --region=${region} \
  --tags "Product=Guardium" \
  --role-only \
  --role-name AmazonEKS_EBS_CSI_DriverRole
```

Let's export that role arn as another env var
```tsx
export ebs_driver_role_arn=$(aws iam list-roles --query 'Roles[?RoleName==`AmazonEKS_EBS_CSI_DriverRole`].Arn' --output text)
```

Create the addon for the cluster

```tsx
eksctl create addon \
--name aws-ebs-csi-driver \
--cluster ${clustername} \
--service-account-role-arn $ebs_driver_role_arn \
--region=${region} \
--force
```

Create the following StorageClass yaml to use gp3

```tsx
cat <<EOF |kubectl apply -f -
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ebs-gp3-sc
provisioner: ebs.csi.aws.com
parameters:
  type: gp3
  fsType: ext4
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
EOF
```

### Verifying EBS 

Run the following command

```tsx
kubectl apply -f - <<EOF 
apiVersion: v1 
kind: PersistentVolumeClaim 
metadata: 
  name: block-pvc 
spec: 
  storageClassName: ebs-gp3-sc
  accessModes: 
    - ReadWriteOnce 
  resources: 
    requests: 
      storage: 1Gi 
--- 
apiVersion: v1 
kind: Pod 
metadata: 
  name: mypod 
spec: 
  containers: 
    - name: myfrontend 
      image: nginx 
      volumeMounts: 
        - mountPath: "/var/www/html" 
          name: mypd 
  volumes: 
    - name: mypd 
      persistentVolumeClaim: 
        claimName: block-pvc 
EOF
```

Verify the PVC was created
```tsx
kubectl get pvc
NAME        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
block-pvc   Bound    pvc-67193212-3e10-46ab-a0a4-2834e3560c4a   1Gi        RWO            ebs-gp3-sc     <unset>                 7s
```

You should see the bound status of the above pvc. 

Delete the test pod and pvc

```tsx
kubectl delete -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: block-pvc
spec:
  storageClassName: ebs-gp3-sc
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
        - mountPath: "/var/www/html"
          name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: block-pvc
EOF
```

### Enable EFS on the cluster

By default when we create a cluster with eksctl it defines and installs `gp2` storage class which is backed by Amazon's EBS (elastic block storage). After that we installed the EBS CSI driver to support `gp3`. However, both `gp2` and `gp3` are both block storage. They will not support RWX in our cluster. We need to install an EFS storage class.

Download the IAM policy document from GitHub. You can also view the [policy document](https://github.com/kubernetes-sigs/aws-efs-csi-driver/blob/master/docs/iam-policy-example.json)
        
```tsx
curl -o iam-policy-example-efs.json https://raw.githubusercontent.com/kubernetes-sigs/aws-efs-csi-driver/master/docs/iam-policy-example.json
```

Create the policy. You can change `AmazonEKS_EFS_CSI_Driver_Policy` to a different name, but if you do, make sure to change it in later steps too.

```tsx
aws iam create-policy \
--policy-name AmazonEKS_EFS_CSI_Driver_Policy \
--tags '{"Key": "Product","Value": "Guardium"}' \
--policy-document file://iam-policy-example-efs.json
```
Output should be similar to below
```tsx
{
    "Policy": {
        "PolicyName": "AmazonEKS_EFS_CSI_Driver_Policy",
        "PolicyId": "ANPA3WENOYSA6LSFRSZ6U",
        "Arn": "arn:aws:iam::803455550593:policy/AmazonEKS_EFS_CSI_Driver_Policy",
        "Path": "/",
        "DefaultVersionId": "v1",
        "AttachmentCount": 0,
        "PermissionsBoundaryUsageCount": 0,
        "IsAttachable": true,
        "CreateDate": "2024-08-23T19:57:13+00:00",
        "UpdateDate": "2024-08-23T19:57:13+00:00",
        "Tags": [
            {
                "Key": "Product",
                "Value": "Guardium"
            }
        ]
    }
}
```

Let's export that policy arn as another env var
```tsx
export efs_driver_policy_arn=$(aws iam list-policies --query 'Policies[?PolicyName==`AmazonEKS_EFS_CSI_Driver_Policy`].Arn' --output text)
```

Create an IAM role and attach the IAM policy to it. Annotate the Kubernetes service account with the IAM role ARN and the IAM role with the Kubernetes service account name. 

```tsx
eksctl create iamserviceaccount \
    --cluster ${clustername} \
    --namespace kube-system \
    --name efs-csi-controller-sa \
    --role-name AmazonEKS_EFS_CSI_DriverRole \
    --attach-policy-arn $efs_driver_policy_arn \
    --tags "Product=Guardium" \
    --approve \
    --region ${region}
```

Once created, check the iam service account is created running the following command.

```bash
eksctl get iamserviceaccount --cluster ${clustername} --region ${region}

```
Should return
```tsx
NAMESPACE	NAME			ROLE ARN
kube-system	ebs-csi-controller-sa	arn:aws:iam::803455550593:role/AmazonEKS_EBS_CSI_DriverRole
kube-system	efs-csi-controller-sa	arn:aws:iam::803455550593:role/AmazonEKS_EFS_CSI_DriverRole
```

Now we just need our add-on registry address. This can be found here: https://docs.aws.amazon.com/eks/latest/userguide/add-ons-images.html

:::note
The add-on registry address is per region. So based on the URL above, since our region is `us-east-1`, then our registry address would be `602401143452.dkr.ecr.us-east-1.amazonaws.com/eks/aws-efs-csi-driver`
:::

Let's install the driver add-on to our clusters. We're going to use `helm` for this.

```tsx
helm repo add aws-efs-csi-driver https://kubernetes-sigs.github.io/aws-efs-csi-driver/

helm repo update
```

Install a release of the driver using the Helm chart. Replace the repository address with the cluster's [container image address](https://docs.aws.amazon.com/eks/latest/userguide/add-ons-images.html).

```tsx
helm upgrade -i aws-efs-csi-driver aws-efs-csi-driver/aws-efs-csi-driver \
    --namespace kube-system \
    --set image.repository=602401143452.dkr.ecr.us-east-1.amazonaws.com/eks/aws-efs-csi-driver \
    --set controller.serviceAccount.create=false \
    --set controller.serviceAccount.name=efs-csi-controller-sa

```

Verify that it installed correctly with this command
```tsx
kubectl get pod -n kube-system -l "app.kubernetes.io/name=aws-efs-csi-driver,app.kubernetes.io/instance=aws-efs-csi-driver"
```

Should return something like
```tsx
NAME                                  READY   STATUS    RESTARTS   AGE
efs-csi-controller-7fc77768fc-2swkw   3/3     Running   0          2m13s
efs-csi-controller-7fc77768fc-c69rb   3/3     Running   0          2m13s
efs-csi-node-ccxns                    3/3     Running   0          2d17h
efs-csi-node-k52pt                    3/3     Running   0          2d17h
efs-csi-node-r8nbm                    3/3     Running   0          2d17h
```

Now we need to create the filesystem in EFS so we can use it

Export the following variables. 

Get our VPC ID
```tsx
vpc_id=$(aws eks describe-cluster \
    --name $clustername \
    --query "cluster.resourcesVpcConfig.vpcId" \
    --region $region \
    --output text)
```
Retrieve the CIDR range for your cluster's VPC and store it in a variable for use in a later step.

```tsx
cidr_range=$(aws ec2 describe-vpcs \
    --vpc-ids $vpc_id \
    --query "Vpcs[].CidrBlock" \
    --output text \
    --region $region)
```
Create a security group with an inbound rule that allows inbound NFS traffic for your Amazon EFS mount points.

```tsx
security_group_id=$(aws ec2 create-security-group \
    --group-name EFS4FileNetSecurityGroup \
    --description "EFS security group for Guardium Insight Clusters" \
    --vpc-id $vpc_id \
    --region $region \
    --output text)
```

Create an inbound rule that allows inbound NFS traffic from the CIDR for your cluster's VPC.

```tsx
aws ec2 authorize-security-group-ingress \
    --group-id $security_group_id \
    --protocol tcp \
    --port 2049 \
    --region $region \
    --cidr $cidr_range
```

Create a file system.
```tsx
file_system_id=$(aws efs create-file-system \
    --region $region \
    --encrypted \
    --tags '{"Key": "Product","Value": "Guardium"}' \
    --performance-mode generalPurpose \
    --query 'FileSystemId' \
    --output text)
```

Create mount targets.

Determine the IDs of the subnets in your VPC and which Availability Zone the subnet is in.
```tsx
aws ec2 describe-subnets \
    --filters "Name=vpc-id,Values=$vpc_id" \
    --query 'Subnets[*].{SubnetId: SubnetId,AvailabilityZone: AvailabilityZone,CidrBlock: CidrBlock}' \
    --region $region \
    --output table
```
Should output something like the following
```tsx
----------------------------------------------------------------------
|                           DescribeSubnets                          |
+------------------+--------------------+----------------------------+
| AvailabilityZone |     CidrBlock      |         SubnetId           |
+------------------+--------------------+----------------------------+
|  us-east-1c      |  192.168.64.0/19   |  subnet-08c33dce5e63c82dc  |
|  us-east-1b      |  192.168.32.0/19   |  subnet-0f7a2b449320cc1e6  |
|  us-east-1a      |  192.168.0.0/19    |  subnet-0ec499ae3eae19eb0  |
|  us-east-1b      |  192.168.128.0/19  |  subnet-04f3d465138687333  |
|  us-east-1a      |  192.168.96.0/19   |  subnet-0bc4d31344c60c113  |
|  us-east-1c      |  192.168.160.0/19  |  subnet-0bee6fc06187cafd1  |
+------------------+--------------------+----------------------------+
```

Add mount targets for the subnets that your nodes are in.

Run the following command:
```bash
for subnet in $(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$vpc_id" --query 'Subnets[*].{SubnetId: SubnetId,AvailabilityZone: AvailabilityZone,CidrBlock: CidrBlock}' --region $region --output text | awk '{print $3}') ; do aws efs create-mount-target --file-system-id $file_system_id --region $region --subnet-id $subnet --security-groups $security_group_id ; done
```

This wraps the below command in a for loop that will iterate through your subnet ids.
```tsx
aws efs create-mount-target \
    --file-system-id $file_system_id \
    --region $region \
    --subnet-id <SUBNETID> \
    --security-groups $security_group_id
```

Create a storage class for dynamic provisioning

Let's get our filesystem ID if we don't already have it above. However if you ran the above steps, `$file_system_id` should already be defined.
```tsx
aws efs describe-file-systems \
--query "FileSystems[*].FileSystemId" \
--region $region \
--output text

fs-071439ffb7e10b67b
```

Download a `StorageClass` manifest for Amazon EFS.
```tsx
curl -o EFSStorageClass.yaml https://raw.githubusercontent.com/kubernetes-sigs/aws-efs-csi-driver/master/examples/kubernetes/dynamic_provisioning/specs/storageclass.yaml

```

:::note[Super Important!]

If you did not export the `$file_system_id` variable, make sure the filesystem id you use in the below command is the filesystem id that was returned to you above!

:::

Update it with the storage class id.
```bash
sed -i "s/fileSystemId:.*/fileSystemId: $file_system_id/" EFSStorageClass.yaml
```

If using the default `sed` command that comes with MacOS

```zsh

sed -i "" "s/fileSystemId:.*/fileSystemId: $file_system_id/" EFSStorageClass.yaml
```

Deploy the storage class.

```bash
kubectl apply -f EFSStorageClass.yaml
```

Set EFS as the default storage class

```bash
kubectl patch storageclass efs-sc -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
```

Finally, verify it's there
```tsx
kubectl get sc
NAME               PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
ebs-gp3-sc         ebs.csi.aws.com         Delete          WaitForFirstConsumer   false                  32m
efs-sc (default)   efs.csi.aws.com         Delete          Immediate              false                  79s
gp2                kubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   false                  71m
```

### Verify EFS

Run the following command to create a pod and a PVC using the EFS storage class

```tsx
kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: block-pvc
spec:
  storageClassName: efs-sc
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
        - mountPath: "/var/www/html"
          name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: block-pvc
EOF
```

Running the following command should verify the PVC was successfully created and bound
```tsx
kubectl get pvc
```

```tsx[OUTPUT]
NAME        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
block-pvc   Bound    pvc-dc711246-02a1-4a9d-b428-a2476b17dd8c   1Gi        RWO            efs-sc         <unset>                 4s
```

### Install NGINX Controller


Let's install the NGINX helm chart
```tsx
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm repo update
```



{/*
Modify the NGINX controller deployment file

Pull down the NGINX controller deployment
```
wget -O nginx-deploy.yaml https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/aws/deploy.yaml
```

Export our relevant subnet ids to a comma delimited env variable

```tsx
export subnets=$(aws eks describe-cluster --name ${clustername} --region ${region} --query "cluster.resourcesVpcConfig.subnetIds" --output json | jq -r 'join(",")' | sed s/,/\\\\,/g)
```
*/}

Create the namespace for NGINX

```tsx
kubectl create ns ingress-nginx
```

Now install the helm chart for nginx

{/*--set-string controller.service.annotations.'service\.beta\.kubernetes\.io/aws-load-balancer-subnets'="$subnets" \ */}

```tsx
helm install ingress-nginx ingress-nginx/ingress-nginx \
--set-string controller.service.annotations.'service\.beta\.kubernetes\.io/aws-load-balancer-backend-protocol'=tcp \
--set-string controller.service.annotations.'service\.beta\.kubernetes\.io/aws-load-balancer-cross-zone-load-balancing-enabled'="true" \
--set-string controller.service.annotations.'service\.beta\.kubernetes\.io/aws-load-balancer-type'=nlb \
--namespace=ingress-nginx

```

Run the following command to verify that a load balancer was assigned

```tsx
kubectl get service --namespace ingress-nginx ingress-nginx-controller
```

```tsx[OUTPUT]
NAME                       TYPE           CLUSTER-IP       EXTERNAL-IP                                                                     PORT(S)                      AGE
ingress-nginx-controller   LoadBalancer   10.100.204.215   a904f47d90a8c4961ae389b155359368-aa0f265b8588c710.elb.us-east-1.amazonaws.com   80:30870/TCP,443:31883/TCP   61s
```

{/*
```tsx
sed "/    service.beta.kubernetes.io\/aws-load-balancer-type: nlb/a\    service.beta.kubernetes.io\/aws-load-balancer-subnets: \"$subnets\"" nginx-deploy.yaml
```

```zsh

```

Modify the deployment file by adding the annotations that do not exist and editing the one(s) that do:
```tsx
service.beta.kubernetes.io/aws-load-balancer-type: "external" // set this to "internal" if not using a public ip
service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: "instance"
service.beta.kubernetes.io/aws-load-balancer-scheme: "internet-facing"
```

Also search for the following configmap entry in the deployment file:

```tsx
---
apiVersion: v1
data:
  allow-snippet-annotations: "false"
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.9.1
  name: ingress-nginx-controller
  namespace: ingress-nginx
---
```
We want to add the following annotations:

```tsx
allow-snippet-annotations: "true"
enable-underscores-in-headers: "true"
nginx.ingress.kubernetes.io/proxy-body-size: "0"
```
So now our config map should look like this
```tsx
---
apiVersion: v1
data:
  allow-snippet-annotations: "false"
kind: ConfigMap
metadata:
  annotations:
    allow-snippet-annotations: "true"
    enable-underscores-in-headers: "true"
    nginx.ingress.kubernetes.io/proxy-body-size: "0" 
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.9.1
  name: ingress-nginx-controller
  namespace: ingress-nginx
---
```

Now apply the deployment
```bash
kubectl apply -f nginx-deploy.yaml
```
*/}
Verify the deployment

Command:

```bash
kubectl get ingressclass
```
Example output:

```tsx
NAME    CONTROLLER             PARAMETERS   AGE
nginx   k8s.io/ingress-nginx   <none>       2m43s
```
{/*
Once created, check the iam service account is created running the following command.

```bash
eksctl get iamserviceaccount --cluster <cluster-name>

NAMESPACE	NAME				ROLE ARN
kube-system	aws-load-balancer-controller	arn:aws:iam::748107796891:role/AmazonEKSLoadBalancerControllerRole
kube-system	efs-csi-controller-sa		arn:aws:iam::748107796891:role/eksctl-filenet-cluster-east-addon-iamserviceaccount-ku-Role1-1SCBRU1DS52QY
```
*/}

### Install the Operator Lifecycle Manager

Run the following command using the `operator-sdk`

```tsx
operator-sdk olm install
```

Verify the installation

```tsx
kubectl get csv -n olm | grep packageserver
```

```tsx[OUTPUT]
packageserver   Package Server   0.28.0               Succeeded
```

### Create the required Namespace

```tsx
kubectl create ns openshift-marketplace
```

Set our context to that namespace and export it as an env var

```tsx
export NAMESPACE=openshift-marketplace

kubectl config set-context --current --namespace $NAMESPACE
```

### Install the IBM Cert Manager

If you followed the directions [here](../01_pre-reqs/#downloading-the-case-file) you should have the `ibm-guardium-insights` case file downloaded and extracted.

Change to the following directory
```tsx
cd ibm-guardium-insights/inventory/install/files/support/eks
```

Create the namespace and then run the installation script

```tsx
kubectl create namespace ibm-cert-manager
chmod +x ibm-cert-manager.sh
./ibm-cert-manager.sh
```

Give it a few minutes and then verify the cert manager is up

```tsx
kubectl get po -n ibm-cert-manager
NAME                                                              READY   STATUS      RESTARTS   AGE
cd6e1c2b84458a4f431f49f499a919d28af0b23693e36f2fc53bc1f2c3hw5pw   0/1     Completed   0          2m1s
cert-manager-cainjector-85777f77cc-hbzg7                          1/1     Running     0          102s
cert-manager-controller-957bc947-kg5wx                            1/1     Running     0          102s
cert-manager-webhook-5586d798f-lcsln                              1/1     Running     0          102s
ibm-cert-manager-catalog-rcbvc                                    1/1     Running     0          2m12s
ibm-cert-manager-operator-64fc5b4644-rnpqg                        1/1     Running     0          108s
```

```tsx
kubectl get csv -n ibm-cert-manager
NAME                               DISPLAY            VERSION   REPLACES   PHASE
ibm-cert-manager-operator.v4.2.7   IBM Cert Manager   4.2.7                Succeeded
```

### Install Foundational Services

Change to the following directory
```tsx
cd ibm-guardium-insights/inventory/install/files/support/eks
```

Export the following vars

```tsx
export REPLACE_NAMESPACE=openshift-marketplace
export NAMESPACE=openshift-marketplace
```

{/*We're going to use the loadbalancer name for this as we aren't using any DNS (hope it works)*/}

We set the domain in our remote dns to `apps.gi.think-forward.work`

```tsx
export HOSTNAME=apps.gi.think-forward.work
```

Now run the CPFS install script

```tsx
chmod +x CPFS-install-template.sh
./CPFS-install-template.sh
```

If you see the following error out, re-run the command
```tsx {10-11}
No resources found in openshift-marketplace namespace.
operatorgroup.operators.coreos.com/operatorgroup created
configmap/ibm-cpp-config created
catalogsource.operators.coreos.com/opencloud-operators created
catalogsource.operators.coreos.com/cloud-native-postgresql-catalog created
subscription.operators.coreos.com/ibm-common-service-operator created
Validate the installation
Waiting for CRD commonservices.operator.ibm.com to be created in namespace openshift-marketplace... Attempt 1/5
CRD commonservices.operator.ibm.com exists.
error: resource mapping not found for name: "common-service" namespace: "openshift-marketplace" from "STDIN": no matches for kind "OperandRequest" in version "operator.ibm.com/v1alpha1"
ensure CRDs are installed first
```

Re-run should return
```tsx
OperatorGroup already exists in namespace openshift-marketplace.
configmap/ibm-cpp-config configured
catalogsource.operators.coreos.com/opencloud-operators unchanged
catalogsource.operators.coreos.com/cloud-native-postgresql-catalog unchanged
subscription.operators.coreos.com/ibm-common-service-operator unchanged
Validate the installation
CRD commonservices.operator.ibm.com exists.
operandrequest.operator.ibm.com/common-service unchanged
```